{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dialogue Inference**\n",
    "\n",
    "TODO:\n",
    "- [x] Wrapper to invoke second (kth) chatting with different parameters\n",
    "    - [x] Defining the inputting features that describes the characteristics of an activator's way of generating texts e.g. invoking the notion of double texting \n",
    "    - [x] Construct Wrapper for this with appropriate integrations with GGUF model \n",
    "    - [x] Learners for Priori's at this stage\n",
    "- [x] Decoding Parameters object by paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Features (Agent & user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planning stage (Drafts)\n",
    "\n",
    "Assumption:\n",
    "- Assuming agent and user is one entity. With this assumption I can safely suppose my data to be the agent's initial states prior to the observing time window. \n",
    "- Transforming human characteristics to frequencies or time series data can be feasible by defining the optimal decoding parameters per time window. For this to be possible, I must consider a mapping function from a set of chosen human characteristics to the length of a text generation. More formally, deriving a unique solution $x$ by solving the following least squares problem existing per time window: \n",
    "    $$\n",
    "    A_{t} x_{t} = b_{t-1}\n",
    "    $$\n",
    "\n",
    "    where $A$ is the matrix representing the human characteristics and $b_{t-1} = ||A_{t-1} x_{t-1} - A_{t} x_{t} ||_2$\n",
    "- Considering the highly probably case of emerging problems related to missing datasets during real-time and uncertainities (at this stage, this shouldn't be a concern and that it is better to assume this is part of the unknown error term in sculpting the cost function)\n",
    "\n",
    "- Modelling methods to deal with this type of problem: \n",
    "    1. Bayesian Ridge Regression \n",
    "    2. Kalman Filtering (Recursive Weighted Least Squares)\n",
    "    3. Gaussian Processes \n",
    "- The idea is that I want to interpret the dialogue / conversational flow as time series \n",
    "- From my countless failures, the previously listed methods won't be used as these methods would be an overkill. Mean-shift methods commonly used in day trading markets is really the most trivial method to apply in this applications. \n",
    "\n",
    "Main Objective\n",
    "- Mapping to AgentDial (Decoding params sampler) to the agent's relative targeting features (e.g. interests range correlates with double texting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file: using device Metal (Apple M1) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from /Users/mimiphan/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/bf5b95e96dac0462e2a09145ec66cae9a3f12067/./Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 13\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q8_0:    2 tensors\n",
      "llama_model_loader: - type q3_K:  128 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Large\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (4.76 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 322 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  4552.64 MiB\n",
      "...............................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x302d602b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x117844710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x30928bfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x302d07c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x30928c980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x30928c410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x30928e730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x302df5110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x30928eeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x307009510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x306ba1cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x306ba1280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x30928f3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x307008f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x30700a3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x309290140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x302d38810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x306ba2d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x306ba3570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x30700b7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x30700bea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x103c185f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x306ba2200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x309290680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x302d31860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x30928fc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x30700af50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x30700c470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x302ddc720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x3092918d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x306ba4290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x306ba4c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x309292190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x309291b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x306ba4ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x306ba54b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x302d3fbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x302d6f510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x3092927d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x302d780e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x302d9ef20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x30700c720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x309293490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x309293bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x306ba5710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x306ba5b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x306ba61a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x306ba6ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x302d1c880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x309294210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x302d3f930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x306ba7020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x103d1f9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x306ba7280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x3092949f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x306ba7b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x306ba74e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x103c18370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x3092950e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x309292d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x309295e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x302d07f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x3092965a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x306ba81a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x306ba8ef0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x302d90180 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x302d26fc0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x309296c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x30700df70 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x309297350 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x3092979f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x302deebd0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x302dd0da0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x30700e1d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x306ba8820 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x309298260 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x30700e430 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x306ba9520 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x3092989e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x309299170 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x302db7040 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x302d1d840 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x309299880 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x306ba9f50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x30700fa90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x30700f3e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x3070108f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x309299ec0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x306baad20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x307010b50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x30929a120 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x306baa810 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x302df5690 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x302d2fee0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x302da2020 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x306bab970 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x30929acb0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x30929a810 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x306ba67d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x306babbd0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x30929baf0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x30929b410 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x306bac8a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x30929c940 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x307010db0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x302d8d2c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x306bacf20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x307011db0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x103c18850 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x306bacb00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x306bae150 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x3070113e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x30929c1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x30929d430 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1178457e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x307011760 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x306baee70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x302dda6d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x30929de60 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x306baf9d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x302dca3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x3070125f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x307012bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x302dab2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x307013db0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10471acb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x3070138a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x307012f80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x3070146c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x306bb0490 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x306bad800 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x30929e0c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x302db1830 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x30929e390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x307015790 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x306bb06f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x306bb1220 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x306bb1aa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x307015e20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x306bb0d40 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117845c30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x30929f260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x30929f870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x3092a0590 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x3070169e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x3092a0ac0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x307016590 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x307015260 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x3092a0d20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x3092a0f80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x302d7dd20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x3092a2090 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x307017070 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x302d13db0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x3070172d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x302d848b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x302d59130 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x302daa1f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x307017600 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x306bb32c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x302d557d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x302d26d00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x302d06f70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x302d58b80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x306bb3ea0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x306bb3520 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x3092a11e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x306bb3780 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x3092a2bd0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x306bb39e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x307017ca0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x306bb55b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x302d49c60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x307017f00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x306bb5810 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x306bb60a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x302d97310 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x3092a1930 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x306bb67d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x306bb6a30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x306bb6f00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x307018160 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x3092a3c30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x3092a4a90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x302df9a30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x302d77e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x3092a51d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x302dfced0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x3070185c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x307019aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x3092a4340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x302debcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x302db0f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x3070195f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x306bb7760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x306bb8ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x306bb9250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x306bb9790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x306bb99f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x3092a5e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x30701aaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x30701b3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x3092a5750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x3092a6630 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x30701baa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x103d20820 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x302d5f7c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x302d761f0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x3092a6f70 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x302da5630 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x3092a7e00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x306bba090 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x306bba5b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x30701c110 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x30701c7b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x306bbae30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x306bbb090 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x306bbb700 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x306bbc7e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x306bbb330 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x3092a8060 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x3092a8450 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x306bbd820 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x3092a8a50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x302dd1300 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x306bbda80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x306bbec20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x3092a9c10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x30701ca10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x306bbe140 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x306bbf760 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x302dcea00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x302dc2290 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x306bbf9c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x30701d030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x30701d6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x30701e1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x302d5f240 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10471af10 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x3092aa280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x302d1fd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x30701eab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x3092aa670 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x3092a8d60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x302d69df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x306bc0710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x302d819e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x3092ab3a0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x3092aa960 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x302d63f20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x306bbfd50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x306bc0ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x3092a6890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x30701f390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x30701ed10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x307020aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x3092ac6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x306bc1d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x306bc2460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x302d561a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x3092abb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x306bc2b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x306bc32a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x302da4800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x302d13200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x3092acc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x3070204b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x3092adac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x3070218c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x103c18f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x3092ae380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x302df9d30 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '13', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_tokenizer import LlamaHFTokenizer\n",
    "from llama_cpp.llama_chat_format import LlamaChatCompletionHandler\n",
    "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
    "\n",
    "MODEL_CARD = \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
    "MODEL_PATH = \"Meta-Llama-3.1-8B-Instruct-Q3_K_XL.gguf\"\n",
    "base_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = LlamaHFTokenizer.from_pretrained(base_model_id)\n",
    "llm = Llama.from_pretrained(MODEL_CARD, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.hf_tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hi! How can I help you today?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.hf_tokenizer.apply_chat_template(\n",
    "    [{'role': 'system', 'content': 'Hello!'}, {'role': 'assistant', 'content': 'Hi! How can I help you today?'}],\n",
    "    tokenize=False,\n",
    "    add_special_tokens=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interests(Enum):\n",
    "    \"\"\" World Topics unique to level of interests. \"\"\"\n",
    "\n",
    "    high: list = ['science', 'programming', 'physics']\n",
    "    medium: list = ['food', 'explore']\n",
    "    low: list = ['biology', 'traveling']\n",
    "\n",
    "class Temp:\n",
    "    def word_tokens(inputs: list, tokenizer):\n",
    "        return tokenizer.hf_tokenizer.encode_plus(\n",
    "            inputs,\n",
    "            is_split_into_words=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m     \n",
      "\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_side\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           PreTrainedTokenizerFast\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "PreTrainedTokenizerFast(name_or_path='meta-llama/Llama-3.1-8B-Instruct', vocab_size=128000, model <...> token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "           }\n",
      "           )\n",
      "\u001b[0;31mLength:\u001b[0m         128256\n",
      "\u001b[0;31mFile:\u001b[0m           /opt/homebrew/Caskroom/miniforge/base/envs/march/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base class for all fast tokenizers (wrapping HuggingFace tokenizers library).\n",
      "\n",
      "Inherits from [`~tokenization_utils_base.PreTrainedTokenizerBase`].\n",
      "\n",
      "Handles all the shared methods for tokenization and special tokens, as well as methods for\n",
      "downloading/caching/loading pretrained tokenizers, as well as adding tokens to the vocabulary.\n",
      "\n",
      "This class also contains the added tokens in a unified way on top of all tokenizers so we don't have to handle the\n",
      "specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).\n",
      "\n",
      "Class attributes (overridden by derived classes)\n",
      "\n",
      "    - **vocab_files_names** (`Dict[str, str]`) -- A dictionary with, as keys, the `__init__` keyword name of each\n",
      "      vocabulary file required by the model, and as associated values, the filename for saving the associated file\n",
      "      (string).\n",
      "    - **pretrained_vocab_files_map** (`Dict[str, Dict[str, str]]`) -- A dictionary of dictionaries, with the\n",
      "      high-level keys being the `__init__` keyword name of each vocabulary file required by the model, the\n",
      "      low-level being the `short-cut-names` of the pretrained models with, as associated values, the `url` to the\n",
      "      associated pretrained vocabulary file.\n",
      "    - **model_input_names** (`List[str]`) -- A list of inputs expected in the forward pass of the model.\n",
      "    - **padding_side** (`str`) -- The default value for the side on which the model should have padding applied.\n",
      "      Should be `'right'` or `'left'`.\n",
      "    - **truncation_side** (`str`) -- The default value for the side on which the model should have truncation\n",
      "      applied. Should be `'right'` or `'left'`.\n",
      "\n",
      "Args:\n",
      "    model_max_length (`int`, *optional*):\n",
      "        The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is\n",
      "        loaded with [`~tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`], this will be set to the\n",
      "        value stored for the associated model in `max_model_input_sizes` (see above). If no value is provided, will\n",
      "        default to VERY_LARGE_INTEGER (`int(1e30)`).\n",
      "    padding_side (`str`, *optional*):\n",
      "        The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      "        Default value is picked from the class attribute of the same name.\n",
      "    truncation_side (`str`, *optional*):\n",
      "        The side on which the model should have truncation applied. Should be selected between ['right', 'left'].\n",
      "        Default value is picked from the class attribute of the same name.\n",
      "    chat_template (`str`, *optional*):\n",
      "        A Jinja template string that will be used to format lists of chat messages. See\n",
      "        https://huggingface.co/docs/transformers/chat_templating for a full description.\n",
      "    model_input_names (`List[string]`, *optional*):\n",
      "        The list of inputs accepted by the forward pass of the model (like `\"token_type_ids\"` or\n",
      "        `\"attention_mask\"`). Default value is picked from the class attribute of the same name.\n",
      "    bos_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
      "        A special token representing the beginning of a sentence. Will be associated to `self.bos_token` and\n",
      "        `self.bos_token_id`.\n",
      "    eos_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
      "        A special token representing the end of a sentence. Will be associated to `self.eos_token` and\n",
      "        `self.eos_token_id`.\n",
      "    unk_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
      "        A special token representing an out-of-vocabulary token. Will be associated to `self.unk_token` and\n",
      "        `self.unk_token_id`.\n",
      "    sep_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
      "        A special token separating two different sentences in the same input (used by BERT for instance). Will be\n",
      "        associated to `self.sep_token` and `self.sep_token_id`.\n",
      "    pad_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
      "        A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n",
      "        attention mechanisms or loss computation. Will be associated to `self.pad_token` and `self.pad_token_id`.\n",
      "    cls_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
      "        A special token representing the class of the input (used by BERT for instance). Will be associated to\n",
      "        `self.cls_token` and `self.cls_token_id`.\n",
      "    mask_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
      "        A special token representing a masked token (used by masked-language modeling pretraining objectives, like\n",
      "        BERT). Will be associated to `self.mask_token` and `self.mask_token_id`.\n",
      "    additional_special_tokens (tuple or list of `str` or `tokenizers.AddedToken`, *optional*):\n",
      "        A tuple or a list of additional special tokens. Add them here to ensure they are skipped when decoding with\n",
      "        `skip_special_tokens` is set to True. If they are not part of the vocabulary, they will be added at the end\n",
      "        of the vocabulary.\n",
      "    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not the model should cleanup the spaces that were added when splitting the input text during the\n",
      "        tokenization process.\n",
      "    split_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the special tokens should be split during the tokenization process. Passing will affect the\n",
      "        internal state of the tokenizer. The default behavior is to not split special tokens. This means that if\n",
      "        `<s>` is the `bos_token`, then `tokenizer.tokenize(\"<s>\") = ['<s>`]. Otherwise, if\n",
      "        `split_special_tokens=True`, then `tokenizer.tokenize(\"<s>\")` will be give `['<','s', '>']`.\n",
      "\n",
      "    tokenizer_object ([`tokenizers.Tokenizer`]):\n",
      "        A [`tokenizers.Tokenizer`] object from 🤗 tokenizers to instantiate from. See [Using tokenizers from 🤗\n",
      "        tokenizers](../fast_tokenizers) for more information.\n",
      "    tokenizer_file ([`str`]):\n",
      "        A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from 🤗\n",
      "        tokenizers.\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      "sequences.\n",
      "\n",
      "Args:\n",
      "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "\n",
      "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      "        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      "        automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      "        automatically.\n",
      "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls padding. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "          sequence if provided).\n",
      "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "          acceptable input length for the model if that argument is not provided.\n",
      "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "          lengths).\n",
      "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls truncation. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "          sequences (or a batch of pairs) is provided.\n",
      "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "          greater than the model maximum admissible input size).\n",
      "    max_length (`int`, *optional*):\n",
      "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "\n",
      "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "    stride (`int`, *optional*, defaults to 0):\n",
      "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "        argument defines the number of overlapping tokens.\n",
      "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "        which it will tokenize. This is useful for NER or token classification.\n",
      "    pad_to_multiple_of (`int`, *optional*):\n",
      "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "        `>= 7.5` (Volta).\n",
      "    padding_side (`str`, *optional*):\n",
      "        The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      "        Default value is picked from the class attribute of the same name.\n",
      "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "\n",
      "    return_token_type_ids (`bool`, *optional*):\n",
      "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are token type IDs?](../glossary#token-type-ids)\n",
      "    return_attention_mask (`bool`, *optional*):\n",
      "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      "        of returning overflowing tokens.\n",
      "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return special tokens mask information.\n",
      "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return `(char_start, char_end)` for each token.\n",
      "\n",
      "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
      "    return_length  (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the lengths of the encoded inputs.\n",
      "    verbose (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to print more information and warnings.\n",
      "    **kwargs: passed to the `self.tokenize()` method\n",
      "\n",
      "Return:\n",
      "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      "\n",
      "    - **input_ids** -- List of token ids to be fed to a model.\n",
      "\n",
      "      [What are input IDs?](../glossary#input-ids)\n",
      "\n",
      "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are token type IDs?](../glossary#token-type-ids)\n",
      "\n",
      "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are attention masks?](../glossary#attention-mask)\n",
      "\n",
      "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      "    - **length** -- The length of the inputs (when `return_length=True`)"
     ]
    }
   ],
   "source": [
    "?tokenizer.hf_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [40657, 92726, 67865], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.hf_tokenizer.encode_plus(Interests.high, is_split_into_words=True, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40657, 92726, 67865]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.hf_tokenizer.encode(Interests.high, is_split_into_words=True, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subclass Initialised params for BeamSearch: <class '__main__.DecodeMethod.BeamSearch'>\n",
      "Checking if an instance has already been made for: `BeamSearch`.  First time creating Instance.\n",
      "BeamSearchParams initialized with temperature=0.5 and top_k=100\n",
      "BeamSearch initialized with decoding params: DecodeMethod.BeamSearch(temperature=0.5, top_k=100, max_tokens=100)\n",
      "Checking if an instance has already been made for: `BeamSearch`.  Instance has already been made and ran total of 1 times.\n",
      "BeamSearchParams initialized with temperature=1.0 and top_k=50\n",
      "BeamSearch initialized with decoding params: DecodeMethod.BeamSearch(temperature=1.0, top_k=50, max_tokens=100)\n",
      "\n",
      "Running experiment with temperature=0.5 and top_k=50\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 50, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 50, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18091.66 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Are you doing anything exciting? I'm doing a lot of work today, unfortunately. I've got a bunch of projects that I need to get done, and I'm trying to get them all finished up. But, I'm also looking forward to the weekend, because I've got a fun trip planned. I'm going to be heading out to a music festival, and I'm really excited to see some of my favorite bands perform live. How about you? What are you doing today?\\nI\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12547.74 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I'm feeling a bit anxious about a work meeting I have later. I've been preparing for it all day, but I'm still worried that I won't be able to communicate my ideas clearly.\\nI'm glad you're feeling prepared for your meeting! It's normal to feel anxious, but try to focus on the positive aspects of the meeting. You've been preparing well, and that's something to be proud of. Take some deep breaths and remember that it's okay to make mistakes.\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 50, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11743.62 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I'm planning on taking a nap. I've been up since 5:30 AM, and I'm feeling a bit tired. I might also do some laundry and tidy up the apartment. How about you? What are your plans for today? I'm planning on taking a nap. I've been up since 5:30 AM, and I'm feeling a bit tired. I might also do some laundry and tidy up the apartment. How about you? What are your plans for today?\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13669.06 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Do you have any plans or are you just going with the flow? Either way, it's a great day to be alive!\\nI'm so glad you asked! I'm actually planning a fun day out with my friends. We're going to hit the beach and soak up some sun. It's going to be a blast!\\nThat sounds amazing! I'm sure you'll have a great time. What are you most looking forward to about your day?\\nI'm really looking forward to swimming and playing\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  Do you have any plans or are you just going with the flow? Either way, it's a great day to be alive!\n",
      "I'm so glad you asked! I'm actually planning a fun day out with my friends. We're going to hit the beach and soak up some sun. It's going to be a blast!\n",
      "That sounds amazing! I'm sure you'll have a great time. What are you most looking forward to about your day?\n",
      "I'm really looking forward to swimming and playing\n",
      "\n",
      "Running experiment with temperature=0.5 and top_k=75\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 75, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 75, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13672.63 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Are you doing something fun, or is it a boring day? Do you have any plans or are you just winging it?\\nI'm doing something fun today! I'm going to the beach! I love spending time at the beach, and I'm so excited to be going today. I'm planning on swimming, building sandcastles, and just enjoying the sunshine.\\nI hope you have a great day, no matter what you're doing. What are your plans for today?\\nI'm going\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13139.23 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" What are your plans? Are you looking forward to anything?\\nI hope you have a great day and that your plans turn out well!\\nI'm glad you're excited about something. It's always nice to have something to look forward to.\\nI'm a bit worried about how the day will go, to be honest. I have a lot on my plate and I'm not sure if I'll be able to get everything done.\\nBut I'm trying to stay positive and focus on the things that I\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 75, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12655.82 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I'm spending the day at home, enjoying some quiet time and catching up on some reading. I hope you have a relaxing day too! #relaxation #reading #homelife\\nA post shared by @lilyrose87 on Mar 18, 2018 at 9:35am PDT\\nThis is a great example of a post that could be used to promote a product or service. The user is highlighting a relaxing day at home, which could be used to promote a\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12817.55 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Do you have any fun plans? I'm excited to share my day with you, so grab a cup of coffee and let's chat!\\n\\nI'm starting the day with a quiet morning at home, catching up on some reading and responding to comments on my blog. After that, I'll be heading out to meet a friend for lunch at a new café in town. We're going to try their specialty sandwiches and catch up on each other's lives. Sounds like a lovely day, doesn't it\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  Do you have any fun plans? I'm excited to share my day with you, so grab a cup of coffee and let's chat!\n",
      "\n",
      "I'm starting the day with a quiet morning at home, catching up on some reading and responding to comments on my blog. After that, I'll be heading out to meet a friend for lunch at a new café in town. We're going to try their specialty sandwiches and catch up on each other's lives. Sounds like a lovely day, doesn't it\n",
      "\n",
      "Running experiment with temperature=0.5 and top_k=100\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 100, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 100, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13435.94 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I'm working on a new blog post for my site, which is all about my favorite travel destinations. I'm also planning a trip to the beach with my friends later this week. How about you? What's on your agenda for today?\\nI'm so glad you're excited about your blog post! I'm sure it will be a great read. As for me, I'm actually planning a surprise party for my best friend's birthday. It's going to be a fun day, but I\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13578.57 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Do you have any plans? I'm taking it easy today, just enjoying the sunshine and the cool breeze. I might do some reading or take a walk later. How about you? What are your plans for the day?\\nI'm glad you're taking it easy today. Sometimes, it's nice to just relax and enjoy the simple things in life. Reading and taking a walk can be great ways to unwind and recharge. Do you have a favorite book or author that you're looking forward to reading\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 100, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13232.79 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I'm going to the library to study and work on a project. I also have a few errands to run, like picking up a package and stopping by the grocery store. How about you? What are your plans for the day?\\nI'm a college student, so I'm always studying and working on projects. I also have a part-time job, so I'm usually busy with that during the week. But on the weekends, I like to take it easy and relax. I'm a\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15512.60 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Do you have any fun plans?\\nI'm actually on vacation right now and I'm spending some quality time with my family. We're going to a water park today and then having a big BBQ dinner tonight. It sounds like it's going to be a fun day!\\nI hope you have a great day too, whatever you're doing. Do you have any fun plans or activities lined up?\\nI'm glad you're having fun on your vacation! Water parks are always a blast, and BBQs\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  Do you have any fun plans?\n",
      "I'm actually on vacation right now and I'm spending some quality time with my family. We're going to a water park today and then having a big BBQ dinner tonight. It sounds like it's going to be a fun day!\n",
      "I hope you have a great day too, whatever you're doing. Do you have any fun plans or activities lined up?\n",
      "I'm glad you're having fun on your vacation! Water parks are always a blast, and BBQs\n",
      "\n",
      "Running experiment with temperature=1.0 and top_k=50\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 50, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 50, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13078.31 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Hopefully you're not stuck inside due to a winter storm. If so, we've got some ideas to keep you entertained.\\nSnow day? Here are 10 ideas to keep you entertained!\\nWe've got some fun ideas to help you enjoy your snow day. Whether you're a movie buff, a reader, or a gamer, we've got something for everyone.\\n1. Watch a classic movie marathon\\nPick your favorite films and binge-watch them. You could watch a trilogy, or even start a\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15061.87 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': ' Is it a normal day or something exciting happening? Or are you feeling a bit down and in need of some motivation?\\nWhatever the day holds for you, I hope you find some inspiration and positivity in these words:\\nToday is a new day, a fresh start. Whatever happened yesterday, or last week, or last year, can be left behind. Today, you have the chance to make a new beginning.\\nYou have the power to choose how you want to live your life. You can choose to', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 50, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13718.10 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I'm doing pretty well, just got done with my morning workout and now I'm thinking about making a big ol' bowl of oatmeal with some fresh fruit and nuts. What about you, do you have any big plans? Maybe hit the gym or get some work done?\\nThe best part about the morning is getting to enjoy a good bowl of oatmeal and a cup of coffee to start my day off right. There's nothing like that feeling of eating something that's good for you and getting\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14666.13 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': ' - I am going to the store to buy some fresh bread\\nWhat are you doing today? - I am going to the store to buy some fresh bread\\nWhat are you doing today? - I am going to the store to buy some fresh bread\\nI love shopping on a sunny day. The fresh bread is going to be delicious.\\nWhat are you doing today? - I am going to the store to buy some fresh bread\\nI love shopping on a sunny day. The fresh bread is going', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  - I am going to the store to buy some fresh bread\n",
      "What are you doing today? - I am going to the store to buy some fresh bread\n",
      "What are you doing today? - I am going to the store to buy some fresh bread\n",
      "I love shopping on a sunny day. The fresh bread is going to be delicious.\n",
      "What are you doing today? - I am going to the store to buy some fresh bread\n",
      "I love shopping on a sunny day. The fresh bread is going\n",
      "\n",
      "Running experiment with temperature=1.0 and top_k=75\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 75, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 75, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13547.76 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Are you busy?\\nYes, I'm a bit busy. I have to finish some projects.\\nI'm doing some homework and studying.\\nI have to finish my presentation for tomorrow's meeting.\\nI have to attend a conference.\\nI'm a bit tired but I still have to finish some work before the end of the day.\\nIt's a bit overwhelming, to be honest.\\nWhat's on your mind? Are you stressed about anything?\\nYes, I'm a bit stressed about this upcoming exam.\\nI\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12566.87 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I hope you're doing something wonderful!\\nI'm doing a blog post about our trip to Hawaii in 2019, where we spent a week on the Big Island. We went to Hawaii Volcanoes National Park and got to see the stunning lava flows up close. It was truly a breathtaking experience! \\nI'm also planning a fun DIY craft project for our upcoming birthday celebration. We're going to make personalized photo frames with special memories and quotes to hang on the wall. I think it will\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 75, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14831.69 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': ' Are you planning a fun day out? Maybe you are getting ready for a big night out?\\nWhatever you have planned, I hope you are getting excited for a great day!\\nI know that life can be unpredictable at times. You might wake up with a plan and then your day is hijacked by unexpected tasks or circumstances.\\nHowever, even on the most unpredictable of days, there is always something you can do to make the most of your time.\\nHere are a few ideas to get you started:\\n', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13001.79 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': ' What’s on your mind? Write about it.\\nI’m sitting at my desk, staring at the blank piece of paper in front of me. The cursor blinks at me, daring me to start writing. But my mind is a jumble of thoughts. There are a thousand things I could write about, but none of them seem to make sense right now.\\n\\nI think about the project I’m working on. It’s been weighing on me for weeks, and I feel like I’m not making', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  What’s on your mind? Write about it.\n",
      "I’m sitting at my desk, staring at the blank piece of paper in front of me. The cursor blinks at me, daring me to start writing. But my mind is a jumble of thoughts. There are a thousand things I could write about, but none of them seem to make sense right now.\n",
      "\n",
      "I think about the project I’m working on. It’s been weighing on me for weeks, and I feel like I’m not making\n",
      "\n",
      "Running experiment with temperature=1.0 and top_k=100\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 100, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 100, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12224.94 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': ' – How to have a Good Day by Caroline A. McGee\\nWhat are you doing today? – How to have a Good Day by Caroline A. McGee\\nWhat are you doing today? – How to have a Good Day by Caroline A. McGee is a simple, yet powerful book that offers practical advice and insights to help you have a good day. The book is divided into four chapters, each focusing on a different aspect of having a good day: focus, energy, relationships,', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11823.21 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Do you have a busy schedule or a relaxing day ahead?\\nI'm planning on a mix of both. I've got some errands to run this morning, but after that, I'll have some time to relax and catch up on some reading.\\nHow about you? What does your day look like?\\nI'm actually going to be doing some research and writing for a school project. I've got a pretty tight deadline, so I'll be putting in some quality time at my desk to make sure\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.0, 'top_k': 100, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11790.02 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Got any fun plans or adventures ahead of you? Or are you just winging it and seeing where the day takes you?\\nPersonally, I've got a pretty packed schedule ahead of me. I've got a bunch of work tasks to tackle, a few video editing projects to finish up, and a long list of errands to run. I'm also hoping to squeeze in some time for a quick workout and a nice lunch break, if I can manage it.\\nI've also got some big plans\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12370.38 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" | M.L.K. Jr. Day\\nToday is a special day, Martin Luther King Jr. Day.\\nDr. King was a civil rights activist, leader, and minister whose powerful voice spoke for the oppressed. He advocated for equal rights for African Americans through nonviolent civil disobedience.\\nTo honor Dr. King's legacy, many Americans are celebrating today with acts of service and community engagement, such as:\\n1. Volunteering at local community organizations or charities.\\n2. Participating in civil\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  | M.L.K. Jr. Day\n",
      "Today is a special day, Martin Luther King Jr. Day.\n",
      "Dr. King was a civil rights activist, leader, and minister whose powerful voice spoke for the oppressed. He advocated for equal rights for African Americans through nonviolent civil disobedience.\n",
      "To honor Dr. King's legacy, many Americans are celebrating today with acts of service and community engagement, such as:\n",
      "1. Volunteering at local community organizations or charities.\n",
      "2. Participating in civil\n",
      "\n",
      "Running experiment with temperature=1.5 and top_k=50\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 50, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 50, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12666.08 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I hope you are enjoying your day as much as I am. I had a quiet morning, doing a bit of research for some of the posts I have scheduled to go up later this week. I also did some editing of a short story that I've been working on, which is coming along nicely, if I do say so myself.\\nI had to spend a bit of time yesterday researching the local area, as I had visitors who wanted to come out to where I live. It was nice to\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12385.28 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I am doing a bit of shopping. The store is having a sale and I want to take advantage of the discounts.\\nWhat kind of things do you need to buy?\\nI need to buy some new clothes, a few groceries, and some household items.\\nIt sounds like you have a lot of errands to run! Yes, I have a lot on my list. I will probably have to make a few trips to the store to get everything I need.\\nDo you think you'll be able to\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 50, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11899.31 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Do you have a long to-do list or a relaxed day ahead of you?\\nI'm hoping to catch up on some writing projects and maybe squeeze in some reading time. But we'll see how that goes!\\nI have a long to-do list, which I'm slowly tackling throughout the day. I hope I can get most of it done, but I also need to take some breaks to rest and recharge. How about you, do you have any fun or important tasks planned for today?\\nI do\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12119.70 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': ' Do you have any big plans or projects? I\\'m just enjoying a quiet morning at home, enjoying a cup of coffee and catching up on some reading. It\\'s been a while since I\\'ve had a moment to myself, and I\\'m savoring it.\\n\\nI hope you have a lovely day and are able to find some moments of peace and tranquility.\\n\\n#coffee #morningroutine #selfcare\\n\\nP.S. I\\'m reading a beautiful book called \"The Nightingale\" by', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  Do you have any big plans or projects? I'm just enjoying a quiet morning at home, enjoying a cup of coffee and catching up on some reading. It's been a while since I've had a moment to myself, and I'm savoring it.\n",
      "\n",
      "I hope you have a lovely day and are able to find some moments of peace and tranquility.\n",
      "\n",
      "#coffee #morningroutine #selfcare\n",
      "\n",
      "P.S. I'm reading a beautiful book called \"The Nightingale\" by\n",
      "\n",
      "Running experiment with temperature=1.5 and top_k=75\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 75, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 75, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11822.78 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" A quick update from a sunny day\\nby admin | Feb 11, 2017 | 0 comments\\nIt's a beautiful day here today. \\xa0The sun is shining and a gentle breeze is rustling through the trees. \\xa0I'm feeling quite inspired, which is great, as I have a lot on my mind to sort out at the moment.\\n\\nAs for what I'm doing today, well, I have a few things on the agenda, but first, I'm going\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12398.32 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Well, you're here to find some information about how to care for your beloved bearded dragon! You've come to the right place!\\nBearded dragons are known for their unique, spiky scales and friendly demeanor. These lovable creatures make great pets for anyone interested in reptile care, but they require special care. We'll cover some important points on how to properly care for your bearded dragon, including their diet, habitat, and health.\\n\\n### 1. **Housing**\\n\\n-\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 75, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12404.79 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I'm so excited to share with you my plans for the day! I've been itching to try out this new coffee shop downtown, so I've planned a little outing with my best friend. We're going to grab coffee, catch up, and then take a little stroll around the neighborhood. Sound like a perfect day to you?\\n\\nAfter we're done with our coffee, I want to try out some new skincare products. I've been eyeing this one brand that everyone raves about, and\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12125.11 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" | What's on your mind?\\nThe way I've been feeling lately has left me questioning everything. It seems like nothing makes sense. Am I doing the right things? Are my choices correct?\\nDo I even know what I want? I've always been someone who values structure and routine. But lately, that's not feeling like an option. What am I supposed to do instead? Where am I supposed to be? Who am I supposed to be with?\\nIt's frustrating because I've never been\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  | What's on your mind?\n",
      "The way I've been feeling lately has left me questioning everything. It seems like nothing makes sense. Am I doing the right things? Are my choices correct?\n",
      "Do I even know what I want? I've always been someone who values structure and routine. But lately, that's not feeling like an option. What am I supposed to do instead? Where am I supposed to be? Who am I supposed to be with?\n",
      "It's frustrating because I've never been\n",
      "\n",
      "Running experiment with temperature=1.5 and top_k=100\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 100, 'max_tokens': 100}\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 100, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12030.82 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': ' Well, if you live in the UK, today is Bonfire Night, also known as Guy Fawkes Night or Fireworks Night. It\\'s a celebration held on November 5th each year to commemorate the failed plot to blow up the English Houses of Parliament in 1605. To mark the occasion, people traditionally set off fireworks, burn effigies (called \"guys\" in the UK), and eat sweet potato and other treats. Not everyone celebrates with fireworks, though – some', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11876.20 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': ' I have a busy day ahead of me. First, I’m going to get some work done. I have to meet with my team and discuss our current projects. Then, I have to take care of some paperwork. And of course, I need to cook dinner for my family tonight.\\nIt sounds like you have a busy day ahead of you. What are your plans after dinner?\\nWell, I have a few errands I need to run. I also have to finish up some other work-related', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 1.5, 'top_k': 100, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11842.49 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I have a pretty packed day ahead of me. I'm going to the park with some friends, then we're heading out for dinner, and after that... well, that's a surprise!\\nI love days like this where I have a mix of socializing and relaxation. I think it's a great way to keep your life interesting and mix things up.\\nDo you have any fun plans or activities lined up for today? Let me know in the comments!\\nA little while ago, I realized that\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12490.86 ms /   101 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" Hopefully something fun. If you're not working, you have a day off to do what you like. The kids are likely in school, or maybe you're playing hooky. Either way, make the most of it.\\nIt's a beautiful day today, I'm told. The sun is shining, the birds are singing, and the air is filled with the sweet scent of blooming flowers. Perfect weather to get outside and enjoy some fresh air.\\nOr, if that's not your thing,\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      "Output returned from experiment run:  Hopefully something fun. If you're not working, you have a day off to do what you like. The kids are likely in school, or maybe you're playing hooky. Either way, make the most of it.\n",
      "It's a beautiful day today, I'm told. The sun is shining, the birds are singing, and the air is filled with the sweet scent of blooming flowers. Perfect weather to get outside and enjoy some fresh air.\n",
      "Or, if that's not your thing,\n"
     ]
    }
   ],
   "source": [
    "# assuming agent's interests are as identical to mine\n",
    "from enum import Enum\n",
    "from functools import wraps\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "\n",
    "class Preference(Enum):\n",
    "    \"\"\" Likingness of a Text. \"\"\"\n",
    "    high = 2\n",
    "    medium = 1\n",
    "    low = 0\n",
    "\n",
    "class SearchMethod(Enum):\n",
    "    greedy = 'greedy'\n",
    "    beam = 'beam'\n",
    "    contrastive = 'contrastive'\n",
    "\n",
    "class SamplingMethod(Enum):\n",
    "    temperature = 'temperature' # or stochastic sampling\n",
    "    top_k = 'top_k'\n",
    "    top_p = 'top_p' # or nucleus sampling\n",
    "    repetition_penalty = 'repetition_penalty'\n",
    "\n",
    "class CherryTopper(Enum):\n",
    "    \"\"\" Addon params to decoding strategies. May or may not be used alone. The Main decoding strategies can live alone without Cherry Toppers but these cannot live without the main decoding strategies. \"\"\"\n",
    "\n",
    "    repetition_penalty = 'repetition_penalty'\n",
    "    length_control = 'length_control'\n",
    "    diversity_penalty = 'diversity_penalty'\n",
    "\n",
    "class ScalingMethod(Enum):\n",
    "    \"\"\" More modern / new approach where the logits are manipulated by penalizing the log probs of the targeted tokens, enabling llm to return words surrounding the regions of these targeted tokens. Similar to CherryTopper except that these can be inferenced alone. Example: Logit Scaler .. \"\"\"\n",
    "    pass\n",
    "\n",
    "def prepareFeatures(features, tokenizer):\n",
    "    output = {}\n",
    "    for key, val in features.__members__.items():\n",
    "        output[key] = tokenizer.hf_tokenizer.encode(val.value, is_split_into_words=True, add_special_tokens=False)\n",
    "    return output\n",
    "\n",
    "class Diversity:\n",
    "    \"\"\" Controls text generations: ['variable', 'creative', 'unexpected', 'surprise', 'uniqueness', 'randomness'].\"\"\"\n",
    "    pass\n",
    "\n",
    "class Coherence:\n",
    "    \"\"\" Controls text generation to adhere with grammatical and contextual information of the prior sequences. \"\"\"\n",
    "    pass\n",
    "\n",
    "class DecodeMethod:\n",
    "    \"\"\" Params by decoding method. \"\"\"\n",
    "    # default state window as the chat window's streaming data\n",
    "    max_tokens: int = 100\n",
    "    stop_strings: list[str] = ['\\n', '\\n\\n']\n",
    "    searching = SearchMethod\n",
    "    sampling = SamplingMethod\n",
    "    scaling = ScalingMethod\n",
    "\n",
    "    @dataclass\n",
    "    class BeamSearch:\n",
    "        temperature: float = 1.0\n",
    "        top_k: int = 50\n",
    "        max_tokens: int = None\n",
    "\n",
    "        def __post_init__(self):\n",
    "            \"\"\"Ensure parameters fall within valid ranges.\"\"\"\n",
    "            self.max_tokens = DecodeMethod.max_tokens\n",
    "            # Validate temperature range\n",
    "            if not (0.0 <= self.temperature <= 5.0):\n",
    "                raise ValueError(f\"temperature must be between {0.0} and {5.0}\")\n",
    "\n",
    "            # Validate top_k range\n",
    "            if not (1 <= self.top_k <= 100):\n",
    "                raise ValueError(f\"top_k must be between {0} and {100}\")\n",
    "\n",
    "            print(f\"BeamSearchParams initialized with temperature={self.temperature} and top_k={self.top_k}\")\n",
    "\n",
    "class AgentDial:\n",
    "    \"\"\" Singleton Base class. \"\"\"\n",
    "    _instances = {}\n",
    "    _counts = {}\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        # Create an instance only if it doesn't exist for the specific arguments\n",
    "        print(f'Checking if an instance has already been made for: `{cls.__name__}`. ', end=' ')\n",
    "\n",
    "        if cls.__name__ not in cls._instances:\n",
    "            print('First time creating Instance.')\n",
    "            instance = super().__new__(cls)\n",
    "            cls._instances[cls.__name__] = instance\n",
    "            cls._counts[cls.__name__] = 0\n",
    "            instance.__initialized = False  # Mark that initialization has not been done yet\n",
    "        else:\n",
    "            print(f'Instance has already been made and ran total of {cls._counts[cls.__name__]} times.')\n",
    "\n",
    "        cls._counts[cls.__name__] += 1\n",
    "        return cls._instances[cls.__name__]\n",
    "\n",
    "    def __call__(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Ensure initialization happens only once\n",
    "            print('Calling from __call__ func and running params generation with: ', end=' ')\n",
    "            kwargs.update(**self.params.__dict__)\n",
    "            print(kwargs, end='\\n')\n",
    "            output = func(*args, **kwargs)\n",
    "            content = {'content': output}\n",
    "            content.update(**kwargs)\n",
    "            self.history.append(content)\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    def __init_subclass__(cls, params, *args, **kwargs):\n",
    "        super().__init_subclass__()\n",
    "        cls.default_params = params\n",
    "        cls.history = []\n",
    "        print(f'Subclass Initialised params for {cls.__name__}: {cls.default_params}')\n",
    "\n",
    "class BeamSearch(AgentDial, params=DecodeMethod.BeamSearch):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = self.default_params(**kwargs) if kwargs else self.default_params()\n",
    "        print(f'{self.__class__.__name__} initialized with decoding params: {self.params}')\n",
    "\n",
    "    def experiment_run(self, user_input, func, temperature_range, top_k_range):\n",
    "        \"\"\"\n",
    "        This method iterates across all combinations of temperature and top_k,\n",
    "        running the given function with each combination.\n",
    "\n",
    "        Parameters:\n",
    "        - func: The function to call with the parameters\n",
    "        - temperature_range: List of temperature values to iterate over\n",
    "        - top_k_range: List of top_k values to iterate over\n",
    "        - user_input: The input that will be passed to the `generate_message` function\n",
    "        TODO:\n",
    "        [ ] Add Features from pre-trained classification models.\n",
    "\n",
    "        \"\"\"\n",
    "        # Get all combinations of temperature and top_k\n",
    "        param_combinations = product(temperature_range, top_k_range)\n",
    "\n",
    "        # Iterate over each combination\n",
    "        caller = self.__call__(func)\n",
    "        for temperature, top_k in param_combinations:\n",
    "            print(f\"\\nRunning experiment with temperature={temperature} and top_k={top_k}\")\n",
    "\n",
    "            # Update the class parameters dynamically\n",
    "            self.params.temperature = temperature\n",
    "            self.params.top_k = top_k\n",
    "            output = caller(user_input)\n",
    "            print('Output returned from experiment run:', output)\n",
    "\n",
    "    __doc__ = \"\"\"\n",
    "    This class initializes the BeamSearch parameters for controlling the beam search decoding method.\n",
    "    - temperature: Controls the randomness of the prediction.\n",
    "    - top_k: Limits the number of top predictions to sample from.\n",
    "    - max_tokens: The maximum number of tokens to generate.\n",
    "\n",
    "    Characteristics (Feature / Combination)\n",
    "    \"\"\"\n",
    "\n",
    "@BeamSearch(temperature=0.5, top_k=100)\n",
    "def generate_message(user_input, **kwargs):\n",
    "    response = llm(user_input, **kwargs)\n",
    "\n",
    "    if set(['id', 'object', 'created', 'model', 'choices', 'usage']) != set(response.keys()):\n",
    "        raise ValueError(f'Incorrect output: {response}')\n",
    "    if not isinstance(response['choices'], list):\n",
    "        raise TypeError(f\"Expected another type of output: {response['choices']}\")\n",
    "    if len(response['choices'][0]) > 2:\n",
    "        print(f\"More than one response was generated. Generated: {response['choices']}\")\n",
    "\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# Define the temperature and top_k ranges\n",
    "temperature_range = [0.5, 1.0, 1.5]\n",
    "top_k_range = [50, 75, 100]\n",
    "\n",
    "# Create an instance of BeamSearch\n",
    "#beam_search_instance = BeamSearch()\n",
    "\n",
    "# Call the experiment_run method to iterate through all combinations and generate responses\n",
    "beam = BeamSearch()\n",
    "beam.experiment_run('What are you doing today?', generate_message, temperature_range, top_k_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if an instance has already been made for: `BeamSearch`.  Instance has already been made and ran total of 2 times.\n",
      "BeamSearchParams initialized with temperature=0.5 and top_k=100\n",
      "BeamSearch initialised with decoding params: DecodeMethod.BeamSearch(temperature=0.5, top_k=100, max_tokens=100)\n",
      "Calling from __call__ func and running params generation with:  {'temperature': 0.5, 'top_k': 100, 'max_tokens': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14163.08 ms /   101 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" \\xa0Do you have a to-do list a mile long? \\xa0Are you feeling stressed or overwhelmed? \\xa0Take a deep breath and let's talk about it.\\nThe first step to managing stress and feeling more productive is to acknowledge how you're feeling. \\xa0Recognize that it's normal to feel stressed and overwhelmed, and that you're not alone. \\xa0Then, take a moment to assess your priorities. \\xa0What needs to be done today? \\xa0What can be\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5404.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11902.80 ms /   101 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one response was generated. Generated: [{'text': \" I hope you have a great day!\\nI'm glad you asked! I'm on a bit of a relaxing day today, which is great. I've been feeling a bit stressed out lately, so it's nice to take a break and unwind. I'm planning on doing some reading, watching some movies, and maybe even doing a bit of crafting. I'm also thinking about cooking up some delicious food for dinner. It's going to be a lovely day, I can tell!\\nHow about you\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n",
      " I hope you have a great day!\n",
      "I'm glad you asked! I'm on a bit of a relaxing day today, which is great. I've been feeling a bit stressed out lately, so it's nice to take a break and unwind. I'm planning on doing some reading, watching some movies, and maybe even doing a bit of crafting. I'm also thinking about cooking up some delicious food for dinner. It's going to be a lovely day, I can tell!\n",
      "How about you\n"
     ]
    }
   ],
   "source": [
    "output = generate_message('What are you doing today?')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \" Are you doing anything exciting? I'm doing a lot of work today, unfortunately. I've got a bunch of projects that I need to get done, and I'm trying to get them all finished up. But, I'm also looking forward to the weekend, because I've got a fun trip planned. I'm going to be heading out to a music festival, and I'm really excited to see some of my favorite bands perform live. How about you? What are you doing today?\\nI\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I'm feeling a bit anxious about a work meeting I have later. I've been preparing for it all day, but I'm still worried that I won't be able to communicate my ideas clearly.\\nI'm glad you're feeling prepared for your meeting! It's normal to feel anxious, but try to focus on the positive aspects of the meeting. You've been preparing well, and that's something to be proud of. Take some deep breaths and remember that it's okay to make mistakes.\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I'm planning on taking a nap. I've been up since 5:30 AM, and I'm feeling a bit tired. I might also do some laundry and tidy up the apartment. How about you? What are your plans for today? I'm planning on taking a nap. I've been up since 5:30 AM, and I'm feeling a bit tired. I might also do some laundry and tidy up the apartment. How about you? What are your plans for today?\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" Are you doing something fun, or is it a boring day? Do you have any plans or are you just winging it?\\nI'm doing something fun today! I'm going to the beach! I love spending time at the beach, and I'm so excited to be going today. I'm planning on swimming, building sandcastles, and just enjoying the sunshine.\\nI hope you have a great day, no matter what you're doing. What are your plans for today?\\nI'm going\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" What are your plans? Are you looking forward to anything?\\nI hope you have a great day and that your plans turn out well!\\nI'm glad you're excited about something. It's always nice to have something to look forward to.\\nI'm a bit worried about how the day will go, to be honest. I have a lot on my plate and I'm not sure if I'll be able to get everything done.\\nBut I'm trying to stay positive and focus on the things that I\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I'm spending the day at home, enjoying some quiet time and catching up on some reading. I hope you have a relaxing day too! #relaxation #reading #homelife\\nA post shared by @lilyrose87 on Mar 18, 2018 at 9:35am PDT\\nThis is a great example of a post that could be used to promote a product or service. The user is highlighting a relaxing day at home, which could be used to promote a\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I'm working on a new blog post for my site, which is all about my favorite travel destinations. I'm also planning a trip to the beach with my friends later this week. How about you? What's on your agenda for today?\\nI'm so glad you're excited about your blog post! I'm sure it will be a great read. As for me, I'm actually planning a surprise party for my best friend's birthday. It's going to be a fun day, but I\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" Do you have any plans? I'm taking it easy today, just enjoying the sunshine and the cool breeze. I might do some reading or take a walk later. How about you? What are your plans for the day?\\nI'm glad you're taking it easy today. Sometimes, it's nice to just relax and enjoy the simple things in life. Reading and taking a walk can be great ways to unwind and recharge. Do you have a favorite book or author that you're looking forward to reading\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I'm going to the library to study and work on a project. I also have a few errands to run, like picking up a package and stopping by the grocery store. How about you? What are your plans for the day?\\nI'm a college student, so I'm always studying and working on projects. I also have a part-time job, so I'm usually busy with that during the week. But on the weekends, I like to take it easy and relax. I'm a\",\n",
       "  'temperature': 0.5,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" Hopefully you're not stuck inside due to a winter storm. If so, we've got some ideas to keep you entertained.\\nSnow day? Here are 10 ideas to keep you entertained!\\nWe've got some fun ideas to help you enjoy your snow day. Whether you're a movie buff, a reader, or a gamer, we've got something for everyone.\\n1. Watch a classic movie marathon\\nPick your favorite films and binge-watch them. You could watch a trilogy, or even start a\",\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': ' Is it a normal day or something exciting happening? Or are you feeling a bit down and in need of some motivation?\\nWhatever the day holds for you, I hope you find some inspiration and positivity in these words:\\nToday is a new day, a fresh start. Whatever happened yesterday, or last week, or last year, can be left behind. Today, you have the chance to make a new beginning.\\nYou have the power to choose how you want to live your life. You can choose to',\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I'm doing pretty well, just got done with my morning workout and now I'm thinking about making a big ol' bowl of oatmeal with some fresh fruit and nuts. What about you, do you have any big plans? Maybe hit the gym or get some work done?\\nThe best part about the morning is getting to enjoy a good bowl of oatmeal and a cup of coffee to start my day off right. There's nothing like that feeling of eating something that's good for you and getting\",\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" Are you busy?\\nYes, I'm a bit busy. I have to finish some projects.\\nI'm doing some homework and studying.\\nI have to finish my presentation for tomorrow's meeting.\\nI have to attend a conference.\\nI'm a bit tired but I still have to finish some work before the end of the day.\\nIt's a bit overwhelming, to be honest.\\nWhat's on your mind? Are you stressed about anything?\\nYes, I'm a bit stressed about this upcoming exam.\\nI\",\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I hope you're doing something wonderful!\\nI'm doing a blog post about our trip to Hawaii in 2019, where we spent a week on the Big Island. We went to Hawaii Volcanoes National Park and got to see the stunning lava flows up close. It was truly a breathtaking experience! \\nI'm also planning a fun DIY craft project for our upcoming birthday celebration. We're going to make personalized photo frames with special memories and quotes to hang on the wall. I think it will\",\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': ' Are you planning a fun day out? Maybe you are getting ready for a big night out?\\nWhatever you have planned, I hope you are getting excited for a great day!\\nI know that life can be unpredictable at times. You might wake up with a plan and then your day is hijacked by unexpected tasks or circumstances.\\nHowever, even on the most unpredictable of days, there is always something you can do to make the most of your time.\\nHere are a few ideas to get you started:\\n',\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': ' – How to have a Good Day by Caroline A. McGee\\nWhat are you doing today? – How to have a Good Day by Caroline A. McGee\\nWhat are you doing today? – How to have a Good Day by Caroline A. McGee is a simple, yet powerful book that offers practical advice and insights to help you have a good day. The book is divided into four chapters, each focusing on a different aspect of having a good day: focus, energy, relationships,',\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" Do you have a busy schedule or a relaxing day ahead?\\nI'm planning on a mix of both. I've got some errands to run this morning, but after that, I'll have some time to relax and catch up on some reading.\\nHow about you? What does your day look like?\\nI'm actually going to be doing some research and writing for a school project. I've got a pretty tight deadline, so I'll be putting in some quality time at my desk to make sure\",\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" Got any fun plans or adventures ahead of you? Or are you just winging it and seeing where the day takes you?\\nPersonally, I've got a pretty packed schedule ahead of me. I've got a bunch of work tasks to tackle, a few video editing projects to finish up, and a long list of errands to run. I'm also hoping to squeeze in some time for a quick workout and a nice lunch break, if I can manage it.\\nI've also got some big plans\",\n",
       "  'temperature': 1.0,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I hope you are enjoying your day as much as I am. I had a quiet morning, doing a bit of research for some of the posts I have scheduled to go up later this week. I also did some editing of a short story that I've been working on, which is coming along nicely, if I do say so myself.\\nI had to spend a bit of time yesterday researching the local area, as I had visitors who wanted to come out to where I live. It was nice to\",\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I am doing a bit of shopping. The store is having a sale and I want to take advantage of the discounts.\\nWhat kind of things do you need to buy?\\nI need to buy some new clothes, a few groceries, and some household items.\\nIt sounds like you have a lot of errands to run! Yes, I have a lot on my list. I will probably have to make a few trips to the store to get everything I need.\\nDo you think you'll be able to\",\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" Do you have a long to-do list or a relaxed day ahead of you?\\nI'm hoping to catch up on some writing projects and maybe squeeze in some reading time. But we'll see how that goes!\\nI have a long to-do list, which I'm slowly tackling throughout the day. I hope I can get most of it done, but I also need to take some breaks to rest and recharge. How about you, do you have any fun or important tasks planned for today?\\nI do\",\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 50,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" A quick update from a sunny day\\nby admin | Feb 11, 2017 | 0 comments\\nIt's a beautiful day here today. \\xa0The sun is shining and a gentle breeze is rustling through the trees. \\xa0I'm feeling quite inspired, which is great, as I have a lot on my mind to sort out at the moment.\\n\\nAs for what I'm doing today, well, I have a few things on the agenda, but first, I'm going\",\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" Well, you're here to find some information about how to care for your beloved bearded dragon! You've come to the right place!\\nBearded dragons are known for their unique, spiky scales and friendly demeanor. These lovable creatures make great pets for anyone interested in reptile care, but they require special care. We'll cover some important points on how to properly care for your bearded dragon, including their diet, habitat, and health.\\n\\n### 1. **Housing**\\n\\n-\",\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I'm so excited to share with you my plans for the day! I've been itching to try out this new coffee shop downtown, so I've planned a little outing with my best friend. We're going to grab coffee, catch up, and then take a little stroll around the neighborhood. Sound like a perfect day to you?\\n\\nAfter we're done with our coffee, I want to try out some new skincare products. I've been eyeing this one brand that everyone raves about, and\",\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 75,\n",
       "  'max_tokens': 100},\n",
       " {'content': ' Well, if you live in the UK, today is Bonfire Night, also known as Guy Fawkes Night or Fireworks Night. It\\'s a celebration held on November 5th each year to commemorate the failed plot to blow up the English Houses of Parliament in 1605. To mark the occasion, people traditionally set off fireworks, burn effigies (called \"guys\" in the UK), and eat sweet potato and other treats. Not everyone celebrates with fireworks, though – some',\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100},\n",
       " {'content': ' I have a busy day ahead of me. First, I’m going to get some work done. I have to meet with my team and discuss our current projects. Then, I have to take care of some paperwork. And of course, I need to cook dinner for my family tonight.\\nIt sounds like you have a busy day ahead of you. What are your plans after dinner?\\nWell, I have a few errands I need to run. I also have to finish up some other work-related',\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100},\n",
       " {'content': \" I have a pretty packed day ahead of me. I'm going to the park with some friends, then we're heading out for dinner, and after that... well, that's a surprise!\\nI love days like this where I have a mix of socializing and relaxation. I think it's a great way to keep your life interesting and mix things up.\\nDo you have any fun plans or activities lined up for today? Let me know in the comments!\\nA little while ago, I realized that\",\n",
       "  'temperature': 1.5,\n",
       "  'top_k': 100,\n",
       "  'max_tokens': 100}]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeamSearch.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.DataFrame(BeamSearch.history).to_parquet('mock_data/beam_search_samples.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BeamSearchParams:\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "    max_token: int = 100\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Ensure parameters fall within valid ranges.\"\"\"\n",
    "        # Validate temperature range\n",
    "        if not (0.0 <= self.temperature <= 5.0):\n",
    "            raise ValueError(f\"temperature must be between {0.0} and {5.0}\")\n",
    "\n",
    "        # Validate top_k range\n",
    "        if not (1 <= self.top_k <= 100):\n",
    "            raise ValueError(f\"top_k must be between {0} and {100}\")\n",
    "\n",
    "        print(f\"BeamSearchParams initialized with temperature={self.temperature} and top_k={self.top_k}\")\n",
    "\n",
    "# Create the BeamSearch decorator\n",
    "def BeamSearcher(temperature, top_k):\n",
    "    \"\"\" Initialize BeamSearch with parameters as a dataclass. \"\"\"\n",
    "    params = BeamSearchParams(temperature, top_k)\n",
    "\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Add BeamSearch parameters to kwargs\n",
    "            kwargs.update({\n",
    "                'temperature': params.temperature,\n",
    "                'top_k': params.top_k\n",
    "            })\n",
    "            print(f\"Running with parameters: temperature={params.temperature}, top_k={params.top_k}\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/1263451/python-decorators-in-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDial(object):\n",
    "    def __init__(self, decode_type: str = 'beam'):\n",
    "        if decode_type == 'beam':\n",
    "            self.decode_sampler = AgentDial.BeamSearch\n",
    "\n",
    "    def __call__(self, func):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: Prepare the Features for mapping to Decoder State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'high': [40657, 92726, 67865],\n",
       " 'medium': [14239, 4683, 20763],\n",
       " 'low': [81162, 49610, 287]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepareFeatures(Interests, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Features based on Human characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dialogue_feats_emotions.py\n",
    "\n",
    "\"\"\"\n",
    "Manual curve fitting user / agent emotions during a chat dialogue with custom degree.\n",
    "\n",
    "Example case scenario:\n",
    "- Positive emotion: +1\n",
    "- Negative emotion: -1\n",
    "- Neutral emotion: 0\n",
    "\n",
    "1. This fits to the observing states of the user's emotions throughout a dialogue over a time interval [t_i, t_j] where i < j and i, j \\in Z^+ representing minutes / hours / seconds / days. The user's expected emotion in the next time interval is predicted using this model.\n",
    "2. Mean square error is used to measure the difference between the expected emotion predicted by my custom model and the returned scores from a model already pretrained on emotion labels (eg. Roberta Emotion logit scores)\n",
    "3. There are 2 ways in implementing this part (really depends on the whole pipeline infastructure)\n",
    "    a.) A new model is trained with the new observed emotion. The mean difference is gathered from step 2 as the target to optimize the weights. Another model is trained using OLS + Gradient Descent where the weights are optimized by minimizing the loss retrieved here. Optionally, this model can be used to represent the entire observations gathered by the agent for relative to this speaker's emotions OR a type of personality\n",
    "    b.) Another method would be automating inferences on the weight's distribution. The distribution of the weights are compared via KL divergence (or other statistical tests). However, further experiments must be fiddled with to validate this method. It is worthy noting that KL divergence is not computationally efficient and should be avoided as much as possible\n",
    "\n",
    "Extensions\n",
    "- The same approach can be used to dynamically building a Bayes Belief Net scales the emotion logit scores with an uncertainity score. The objective of this extension is that the predictions improve over time by incooperating the uncertainity score and frequency of the observed state. The uncertainty score is formulated by counting the discrepancies between the temporary/cached data and the observations\n",
    "- Combined with the forementioned point, this method can be used to flag other human behaviour analytics such as a user's sarcasm\n",
    "- Unecessary extension would be incooperating the triginometric transformation to the predicting y data. This is to leverage the odd and even characterstics of the sin and cos function, respectively. Long term objective is fouriers transformation hell yea\n",
    "- Instead of the scenario used in this case, consider this model for flagging how emotional the response is (regardless of it being negative or positive) against a neutral response.\n",
    "- Use this in updating a Graph network that stores responses relative to input sequence in Redis.\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "DEFAULT_CHAT_INTERVAL = 10\n",
    "margin = 0.15\n",
    "degree = 9\n",
    "\n",
    "def fit(y: np.ndarray, fn: bool = False):\n",
    "    if fn is True:\n",
    "        y = np.tanh(y)\n",
    "\n",
    "    x = np.arange(DEFAULT_CHAT_INTERVAL + 1)\n",
    "    X = np.vander(x, degree+1, increasing=True)\n",
    "    coefficients, residuals, rank, eigen = np.linalg.lstsq(X, y, rcond=None)\n",
    "    print(f\"Coefficients: {coefficients}\\nResidual: {residuals}\\nEigen: {eigen}\")\n",
    "\n",
    "    return coefficients, y\n",
    "\n",
    "def pred(coefficients: np.ndarray, future_min: int, future_max: int):\n",
    "    pred_interval = np.arange(future_min, future_max)\n",
    "    x = np.vander(pred_interval, degree + 1, increasing=True)\n",
    "    return np.dot(x, coefficients)\n",
    "\n",
    "def test(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    # if predicting y values are [-1, 1]\n",
    "    y = np.where(abs(1 - y_pred) <= margin, 0, y_pred)\n",
    "    #y_new = np.where(y_pred > 0, 1, -1)\n",
    "    return np.sum(np.where(y == y_true, 1, 0)) / len(y_true)\n",
    "\n",
    "def mse_loss(y_pred: np.ndarray, y_true: np.ndarray):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "y = np.random.choice([-1, 0, 1], size=DEFAULT_CHAT_INTERVAL+1) # tb replaced\n",
    "coeff, y = fit(y=y, fn=False)\n",
    "y_pred = pred(coeff, 0, 11)\n",
    "print(f\"Y predicted: {np.round(y_pred, 4)}\\nY true: {y}\\nMSE: {np.round(mse_loss(y_pred=y_pred, y_true=y), 4)}\\nTesting: {test(y_pred, y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Humour "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be updated......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "march",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
